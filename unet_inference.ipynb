{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3178120-25e1-4a1d-88e2-4b9a74c8124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14fe7a16-b67b-4d7d-a317-2508e103a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UNet model class\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Define the contracting (encoder) path\n",
    "        self.encoder1 = self.contracting_block(3, 64)\n",
    "        self.encoder2 = self.contracting_block(64, 128)\n",
    "        self.encoder3 = self.contracting_block(128, 256)\n",
    "        self.encoder4 = self.contracting_block(256, 512)\n",
    "        self.encoder5 = self.contracting_block(512, 1024)\n",
    "\n",
    "        # Define the expansive (decoder) path\n",
    "        self.decoder1 = self.expansive_block(1024, 512)\n",
    "        self.decoder2 = self.expansive_block(1024, 256)  # Input from decoder1 + encoder4\n",
    "        self.decoder3 = self.expansive_block(512, 128)  # Input from decoder2 + encoder3\n",
    "        self.decoder4 = self.expansive_block(256, 64)   # Input from decoder3 + encoder2\n",
    "        self.final_layer = nn.Conv2d(128, 1, kernel_size=1)  # Input from decoder4 + encoder1\n",
    "\n",
    "    def contracting_block(self, in_channels, out_channels):\n",
    "        # Contracting block with two convolutional layers followed by batch normalization and ReLU activation\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def expansive_block(self, in_channels, out_channels):\n",
    "        # Expansive block with two convolutional layers followed by batch normalization and ReLU activation\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        # Crop and concatenate function to handle the skip connections\n",
    "        diffY = bypass.size()[2] - upsampled.size()[2]\n",
    "        diffX = bypass.size()[3] - upsampled.size()[3]\n",
    "        upsampled = F.pad(upsampled, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2))\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(F.max_pool2d(e1, kernel_size=2, stride=2))\n",
    "        e3 = self.encoder3(F.max_pool2d(e2, kernel_size=2, stride=2))\n",
    "        e4 = self.encoder4(F.max_pool2d(e3, kernel_size=2, stride=2))\n",
    "        e5 = self.encoder5(F.max_pool2d(e4, kernel_size=2, stride=2))\n",
    "\n",
    "        # Decoder path\n",
    "        d1 = self.crop_and_concat(F.interpolate(self.decoder1(F.interpolate(e5, scale_factor=2, mode='bilinear')), scale_factor=2, mode='bilinear'), e4)\n",
    "        d2 = self.crop_and_concat(F.interpolate(self.decoder2(F.interpolate(d1, scale_factor=2, mode='bilinear')), scale_factor=2, mode='bilinear'), e3)\n",
    "        d3 = self.crop_and_concat(F.interpolate(self.decoder3(F.interpolate(d2, scale_factor=2, mode='bilinear')), scale_factor=2, mode='bilinear'), e2)\n",
    "        d4 = self.crop_and_concat(F.interpolate(self.decoder4(F.interpolate(d3, scale_factor=2, mode='bilinear')), scale_factor=2, mode='bilinear'), e1)\n",
    "\n",
    "        # Final output layer\n",
    "        final_output = self.final_layer(d4)\n",
    "        return final_output\n",
    "\n",
    "model = UNet()  # Instantiate the UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782e3b38-e14b-40a1-b16a-7e2c1d192156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model outputs range: -7.473902702331543 to 0.8656378388404846\n",
      "Unique values in the binary mask: [0 1]\n",
      "Inference on image completed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transformations for inference\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),              # Convert numpy array to PIL image\n",
    "    transforms.Resize((256, 256)),        # Resize the image to 256x256 pixels\n",
    "    transforms.ToTensor(),                # Convert PIL image to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the image with mean and std deviation of 0.5\n",
    "])\n",
    "\n",
    "# Define the function to perform inference on a single image\n",
    "def inference_on_image(model, image_path, transform, device):\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path)                       # Read the image from the specified path\n",
    "    original_size = image.shape[:2]                      # Save original image size (height, width)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)       # Convert the image from BGR to RGB\n",
    "    input_image = transform(image).unsqueeze(0).to(device)  # Apply transformations and add batch dimension\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():                                # Disable gradient computation\n",
    "        output = model(input_image)                      # Get the model output\n",
    "        \n",
    "        # Check and print raw model outputs range\n",
    "        raw_output = output.cpu().numpy().squeeze()      # Convert the output to numpy array and remove batch dimension\n",
    "        print(f\"Raw model outputs range: {raw_output.min()} to {raw_output.max()}\")\n",
    "        \n",
    "        output = torch.sigmoid(output)                   # Apply sigmoid to get probability map\n",
    "        \n",
    "        # Visualize the raw output mask\n",
    "        raw_output_image = (output.cpu().numpy().squeeze() * 255).astype(np.uint8)  # Scale the output to 0-255 range and convert to uint8\n",
    "        raw_output_image = cv2.resize(raw_output_image, (original_size[1], original_size[0]), interpolation=cv2.INTER_NEAREST)  # Resize to original image size\n",
    "        cv2.imwrite('raw_output_image.jpg', raw_output_image)  # Save the raw output image\n",
    "        \n",
    "        # Apply thresholding to get binary mask\n",
    "        binary_output = (output.cpu().numpy().squeeze() > 0.5).astype(np.uint8)  # Threshold the output at 0.5\n",
    "        \n",
    "        # Check and print unique values in the binary output\n",
    "        unique_values = np.unique(binary_output)         # Get unique values in the binary mask\n",
    "        print(f\"Unique values in the binary mask: {unique_values}\")\n",
    "        \n",
    "        # Visualize the binary output mask\n",
    "        binary_output_image = (binary_output * 255).astype(np.uint8)  # Scale the binary mask to 0-255 range and convert to uint8\n",
    "        binary_output_image = cv2.resize(binary_output_image, (original_size[1], original_size[0]), interpolation=cv2.INTER_NEAREST)  # Resize to original image size\n",
    "        cv2.imwrite('binary_output_image.jpg', binary_output_image)  # Save the binary output image\n",
    "    \n",
    "    # Resize the binary output mask to the original image size\n",
    "    output_resized = cv2.resize(binary_output, (original_size[1], original_size[0]), interpolation=cv2.INTER_NEAREST)  # Resize to original image size\n",
    "    \n",
    "    # Superimpose the result on the original image\n",
    "    superimposed_image = image.copy()                   # Copy the original image\n",
    "    superimposed_image[output_resized == 1] = [0, 255, 0]  # Highlight drivable area with green color\n",
    "    \n",
    "    return superimposed_image\n",
    "\n",
    "# Perform inference and save the result\n",
    "model.load_state_dict(torch.load('unet_model.pth'))      # Load the trained model weights\n",
    "model.eval()                                             # Set the model to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available, else use CPU\n",
    "model.to(device)                                         # Move the model to the specified device\n",
    "input_image_path = 'road_image_191.png'  # Replace with your image path\n",
    "result_image = inference_on_image(model, input_image_path, transform, device)  # Perform inference on the input image\n",
    "\n",
    "# Save the superimposed image\n",
    "result_image = Image.fromarray(result_image)             # Convert numpy array to PIL image\n",
    "result_image.save('superimposed_image.jpg')              # Save the superimposed image\n",
    "print(\"Inference on image completed and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e9563e-166c-4544-a8ae-254ac18f5954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|████████████████████| 391/391 [00:08<00:00, 44.08frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference on video completed and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference on a Video\n",
    "def inference_on_video(model, video_path, transform, device):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('output_video.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get total number of frames for progress bar\n",
    "    with tqdm(total=total_frames, desc=\"Processing Video\", unit=\"frame\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                # Save original frame size\n",
    "                original_size = frame.shape[:2]\n",
    "                \n",
    "                # Preprocess the frame\n",
    "                input_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                input_frame = transform(input_frame).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Perform inference\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_frame)\n",
    "                    output = torch.sigmoid(output)\n",
    "                    output = output.cpu().numpy().squeeze()\n",
    "                    output = (output > 0.5).astype(np.uint8)\n",
    "                \n",
    "                # Resize the output mask to the original frame size\n",
    "                output_resized = cv2.resize(output, (original_size[1], original_size[0]), interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                # Superimpose the result on the original frame\n",
    "                superimposed_frame = frame.copy()\n",
    "                superimposed_frame[output_resized == 1] = [0, 255, 0]  # Highlight drivable area with green\n",
    "                \n",
    "                # Write the superimposed frame to the output video\n",
    "                out.write(superimposed_frame)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(\"Inference on video completed and saved!\")\n",
    "\n",
    "# Perform inference on the video\n",
    "input_video_path = 'video_road_driving.mp4'  # Replace with your video path\n",
    "inference_on_video(model, input_video_path, transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ac078-6da4-4314-bb16-f52808856d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
